{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assignemnt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',transform=lambda x: np.array(x).flatten(),download=True,train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    encoded_labels = np.zeros((len(labels), 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i][label] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "train_Y = encode_labels(train_Y)\n",
    "test_Y = encode_labels(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return data / 255\n",
    "\n",
    "train_X = normalize_data(train_X)\n",
    "test_X = normalize_data(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.2627451  0.90980392\n",
      " 0.15294118 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.24313725 0.31764706\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.47058824 0.70588235 0.15294118 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.49411765 0.63921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00784314\n",
      " 0.6        0.82352941 0.15686275 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8627451  0.63921569 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10588235 0.99607843 0.63529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.87058824 0.63921569\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.71764706 0.99607843 0.49019608 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.18039216 0.96078431 0.63921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.77647059\n",
      " 0.99607843 0.21960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.47058824\n",
      " 0.99607843 0.63921569 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.90588235 0.99607843 0.11372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.62352941 0.99607843 0.47058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.63921569 0.99607843 0.84705882 0.0627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.62352941 0.99607843 0.2627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.3372549  0.69803922 0.97254902 0.99607843\n",
      " 0.35686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.62352941\n",
      " 0.99607843 0.33333333 0.         0.         0.         0.18431373\n",
      " 0.19215686 0.45490196 0.56470588 0.58823529 0.94509804 0.95294118\n",
      " 0.91764706 0.70196078 0.94509804 0.98823529 0.15686275 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.58823529 0.99215686 0.92941176\n",
      " 0.81176471 0.81176471 0.81176471 0.99215686 0.99607843 0.98039216\n",
      " 0.94117647 0.77647059 0.56078431 0.35686275 0.10980392 0.01960784\n",
      " 0.91372549 0.98039216 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.46666667 0.69411765 0.69411765 0.69411765\n",
      " 0.69411765 0.69411765 0.38431373 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.4        0.99607843 0.8627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  0.99607843 0.5372549  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.6627451\n",
      " 0.99607843 0.22352941 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6627451  0.99607843 0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  1.         0.36862745 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.6627451\n",
      " 0.99607843 0.37647059 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6627451  0.99607843 0.6\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  1.         0.6        0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37647059\n",
      " 0.99607843 0.6        0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_X[2])\n",
    "print(train_Y.shape)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(data, labels, batch_size):\n",
    "    batches = []\n",
    "    permutatuion = np.random.permutation(data.shape[0])\n",
    "    data_shuffled = data[permutatuion]\n",
    "    labels_shuffled = labels[permutatuion]\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        batches.append((data_shuffled[i:i+batch_size], labels_shuffled[i:i+batch_size]))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b):\n",
    "    z= np.dot(X, W) + b\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(true_label, predicetd_labels):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y_true, y_pred, W, b, learning_rate):\n",
    "    size = X.shape[0] \n",
    "    dW = np.dot(X.T, (y_pred - y_true)) / size\n",
    "    db = np.sum(y_pred - y_true, axis=0) / size\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch,batch_labels,weights,bias,learning_rate):\n",
    "    y_pred = forward_propagation(batch, weights, bias)\n",
    "    weights, bias = gradient_descent(batch, batch_labels, y_pred, weights, bias, learning_rate)\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data,labels,learning_rate,epochs,batch_size):\n",
    "    weights = np.random.rand(training_data.shape[1], 10)\n",
    "    biases = np.zeros(10)\n",
    "    for _ in range(epochs):\n",
    "        batches = split_into_batches(training_data, labels, batch_size)\n",
    "        for batch, batch_labels in batches:\n",
    "            weights, biases = train_batch(batch, batch_labels, weights, biases, learning_rate)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights,biases = train(train_X,train_Y,0.01,100,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights2.npy', weights)\n",
    "np.save('biases2.npy', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data,labels,weights,biases):\n",
    "    predictions = forward_propagation(data, weights, biases)\n",
    "    predicted= np.argmax(predictions, axis=1)\n",
    "    actual = np.argmax(labels, axis=1)\n",
    "    accuracy = np.sum(predicted == actual) / data.shape[0]\n",
    "    return accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9158)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_X,test_Y,weights,biases)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
