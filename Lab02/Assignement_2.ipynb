{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assignemnt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',transform=lambda x: np.array(x).flatten(),download=True,train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    encoded_labels = np.zeros((len(labels), 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i][label] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "train_Y = encode_labels(train_Y)\n",
    "test_Y = encode_labels(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return data / 255\n",
    "\n",
    "train_X = normalize_data(train_X)\n",
    "test_X = normalize_data(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.2627451  0.90980392\n",
      " 0.15294118 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.24313725 0.31764706\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.47058824 0.70588235 0.15294118 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.49411765 0.63921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00784314\n",
      " 0.6        0.82352941 0.15686275 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8627451  0.63921569 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10588235 0.99607843 0.63529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.87058824 0.63921569\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.71764706 0.99607843 0.49019608 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.18039216 0.96078431 0.63921569 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.77647059\n",
      " 0.99607843 0.21960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.47058824\n",
      " 0.99607843 0.63921569 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.90588235 0.99607843 0.11372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.62352941 0.99607843 0.47058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.63921569 0.99607843 0.84705882 0.0627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.62352941 0.99607843 0.2627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.3372549  0.69803922 0.97254902 0.99607843\n",
      " 0.35686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.62352941\n",
      " 0.99607843 0.33333333 0.         0.         0.         0.18431373\n",
      " 0.19215686 0.45490196 0.56470588 0.58823529 0.94509804 0.95294118\n",
      " 0.91764706 0.70196078 0.94509804 0.98823529 0.15686275 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.58823529 0.99215686 0.92941176\n",
      " 0.81176471 0.81176471 0.81176471 0.99215686 0.99607843 0.98039216\n",
      " 0.94117647 0.77647059 0.56078431 0.35686275 0.10980392 0.01960784\n",
      " 0.91372549 0.98039216 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.46666667 0.69411765 0.69411765 0.69411765\n",
      " 0.69411765 0.69411765 0.38431373 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.4        0.99607843 0.8627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  0.99607843 0.5372549  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.6627451\n",
      " 0.99607843 0.22352941 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6627451  0.99607843 0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  1.         0.36862745 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.6627451\n",
      " 0.99607843 0.37647059 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6627451  0.99607843 0.6\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.6627451  1.         0.6        0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37647059\n",
      " 0.99607843 0.6        0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_X[2])\n",
    "print(train_Y.shape)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(data, labels, batch_size):\n",
    "    batches = []\n",
    "    permutatuion = np.random.permutation(data.shape[0])\n",
    "    data_shuffled = data[permutatuion]\n",
    "    labels_shuffled = labels[permutatuion]\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        batches.append((data_shuffled[i:i+batch_size], labels_shuffled[i:i+batch_size]))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b):\n",
    "    z= np.dot(X, W) + b\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y_true, y_pred, W, b, learning_rate):\n",
    "    size = X.shape[0] \n",
    "    dW = np.dot(X.T, (y_pred - y_true)) / size\n",
    "    db = np.sum(y_pred - y_true, axis=0) / size\n",
    "    \n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch,batch_labels,weights,bias,learning_rate):\n",
    "    y_pred = forward_propagation(batch, weights, bias)\n",
    "    weights, bias = gradient_descent(batch, batch_labels, y_pred, weights, bias, learning_rate)\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights,biases = train(train_X,train_Y,0.01,100,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights2.npy', weights)\n",
    "np.save('biases2.npy', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data,labels,weights,biases):\n",
    "    predictions = forward_propagation(data, weights, biases)\n",
    "    predicted= np.argmax(predictions, axis=1)\n",
    "    actual = np.argmax(labels, axis=1)\n",
    "    accuracy = np.sum(predicted == actual) / data.shape[0]\n",
    "    return accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9158)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_X,test_Y,weights,biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def train(training_data,labels,learning_rate,epochs,batch_size,accuracyFunction):\n",
    "    weights = np.random.rand(training_data.shape[1], 10)\n",
    "    biases = np.zeros(10)\n",
    "    start = time()\n",
    "    for epoch in range(epochs):\n",
    "        batches = split_into_batches(training_data, labels, batch_size)\n",
    "        for batch, batch_labels in batches:\n",
    "            weights, biases = train_batch(batch, batch_labels, weights, biases, learning_rate)\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            duration = time() - start\n",
    "            accuracy = accuracyFunction(weights, biases) *100\n",
    "            print(f'Epoch {epoch+1}/{epochs} - Accuracy: {accuracy:.2f}% | Duration: {duration:.2f}s')\n",
    "            np.save(f'models/weights_{epoch+1}.npy', weights)\n",
    "            np.save(f'models/biases_{epoch+1}.npy', biases)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500 - Accuracy: 91.06% | Duration: 34.69s\n",
      "Epoch 100/500 - Accuracy: 91.65% | Duration: 76.54s\n",
      "Epoch 150/500 - Accuracy: 91.87% | Duration: 118.79s\n",
      "Epoch 200/500 - Accuracy: 92.04% | Duration: 160.42s\n",
      "Epoch 250/500 - Accuracy: 92.22% | Duration: 197.20s\n",
      "Epoch 300/500 - Accuracy: 92.26% | Duration: 233.60s\n",
      "Epoch 350/500 - Accuracy: 92.30% | Duration: 268.97s\n",
      "Epoch 400/500 - Accuracy: 92.31% | Duration: 304.06s\n",
      "Epoch 450/500 - Accuracy: 92.37% | Duration: 339.47s\n",
      "Epoch 500/500 - Accuracy: 92.40% | Duration: 374.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.62648234, 0.9877174 , 0.93342912, ..., 0.6201964 , 0.96479561,\n",
       "         0.60056878],\n",
       "        [0.13288015, 0.75012825, 0.77921429, ..., 0.50864505, 0.27296994,\n",
       "         0.92911452],\n",
       "        [0.24094357, 0.3005735 , 0.81222496, ..., 0.57551565, 0.16132618,\n",
       "         0.43317931],\n",
       "        ...,\n",
       "        [0.35517075, 0.18327582, 0.85509174, ..., 0.31068536, 0.25400309,\n",
       "         0.47066609],\n",
       "        [0.15211833, 0.10667057, 0.24110797, ..., 0.90491558, 0.47958982,\n",
       "         0.36430446],\n",
       "        [0.91854232, 0.83102895, 0.83656544, ..., 0.33149008, 0.74249617,\n",
       "         0.36807191]]),\n",
       " array([-0.94600694,  0.61915391,  0.22557197, -0.51574565,  0.12335081,\n",
       "         2.17994722, -0.3081909 ,  1.23631833, -2.20442952, -0.40996922]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_X,train_Y,0.01,500,100, lambda w, b: test(test_X, test_Y, w, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
