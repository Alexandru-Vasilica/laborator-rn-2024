{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assignemnt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',transform=lambda x: np.array(x).flatten(),download=True,train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    encoded_labels = np.zeros((len(labels), 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i][label] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "train_Y = encode_labels(train_Y)\n",
    "test_Y = encode_labels(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return data / 255\n",
    "\n",
    "train_X = normalize_data(train_X)\n",
    "test_X = normalize_data(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_Y.shape)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(data, labels, batch_size):\n",
    "    batches = []\n",
    "    permutatuion = np.random.permutation(data.shape[0])\n",
    "    data_shuffled = data[permutatuion]\n",
    "    labels_shuffled = labels[permutatuion]\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        batches.append((data_shuffled[i:i+batch_size], labels_shuffled[i:i+batch_size]))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    clipped = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-clipped))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu_prime(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b,dropout_rate=None):\n",
    "    activations = [X]\n",
    "    zs=[]\n",
    "    for w,b in zip(W, b):\n",
    "        z = np.dot(activations[-1], w) + b\n",
    "        activation= relu(z)\n",
    "        if dropout_rate:\n",
    "            mask = (np.random.rand(*activation.shape) > dropout_rate).astype(float)\n",
    "            z = z * mask\n",
    "            activation = activation * mask / (1 - dropout_rate)\n",
    "        zs.append(z)\n",
    "        activations.append(activation)\n",
    "    activations[-1] = softmax(zs[-1])\n",
    "    return activations, zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(W, b, t, activations, zs):\n",
    "   batch_size = t.shape[0]\n",
    "   gradient_b = [np.zeros(b.shape) for b in b]\n",
    "   gradient_W = [np.zeros(w.shape) for w in W]\n",
    "   error = activations[-1] - t\n",
    "   for layer in range(len(W) - 1, -1, -1):\n",
    "       gradient_b[layer] = np.mean(error, axis=0, keepdims=True)\n",
    "       gradient_W[layer] = np.dot(activations[layer].T, error) / batch_size\n",
    "       if layer > 0:  \n",
    "            error = np.dot(error, W[layer].T) * relu_prime(zs[layer-1])\n",
    "   return gradient_W, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(initial_lr, epoch, decay_rate=0.1, decay_steps=10):\n",
    "    return initial_lr * (1 / (1 + decay_rate * (epoch // decay_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y_true, y_pred, W, b, learning_rate):\n",
    "    size = X.shape[0] \n",
    "    dW = np.dot(X.T, (y_pred - y_true)) / size\n",
    "    db = np.sum(y_pred - y_true, axis=0) / size\n",
    "    \n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch,batch_labels,weights,biases,learning_rate,dropout_rate=None):\n",
    "      activations, zs = forward_propagation(batch, weights, biases,dropout_rate)\n",
    "      gradient_W, gradient_b = backward_propagation(weights, biases, batch_labels, activations, zs)\n",
    "      for i in range(len(weights)):\n",
    "          weights[i], biases[i] = weights[i] - learning_rate * gradient_W[i], biases[i] - learning_rate * gradient_b[i]\n",
    "      return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layers(input_size, hidden_layers, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layers = [input_size] + hidden_layers + [output_size]\n",
    "  \n",
    "    for i in range(1, len(layers)):\n",
    "        fan_in = layers[i-1]\n",
    "        fan_out = layers[i]\n",
    "        W=np.random.randn(fan_in,fan_out) * np.sqrt(2 / fan_in)\n",
    "        b = np.ones((1, fan_out)) * 0.1\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data,labels,weights,biases):\n",
    "    activations,zs = forward_propagation(data, weights, biases)\n",
    "    predictions = np.argmax(activations[-1], axis=1)\n",
    "    true_labels = np.argmax(labels, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    return accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_model(weights, biases,epochs):\n",
    "    os.makedirs(f'models_dropout/epochs_{epochs}', exist_ok=True)\n",
    "    for layer in range(len(weights)):\n",
    "        np.save(f'models_dropout/epochs_{epochs}/weights_{layer+1}.npy', weights[layer])\n",
    "        np.save(f'models_dropout/epochs_{epochs}/biases_{layer+1}.npy', biases[layer])\n",
    "\n",
    "def load_model(epochs,layers):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(layers):\n",
    "        weights.append(np.load(f'models_dropout/epochs_{epochs}/weights_{i+1}.npy'))\n",
    "        biases.append(np.load(f'models_dropout/epochs_{epochs}/biases_{i+1}.npy'))\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Hyperparameters(TypedDict):\n",
    "    learning_rate: float\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    hidden_layers: list[int]\n",
    "    dropout_rate: float | None\n",
    "    patience: int\n",
    "    reduce_factor: float\n",
    "    min_lr: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_validation(data, labels, ratio):\n",
    "    permutation = np.random.permutation(data.shape[0])\n",
    "    validation_size = int(data.shape[0] * ratio)\n",
    "    validation_indices = permutation[:validation_size]\n",
    "    training_indices = permutation[validation_size:]\n",
    "    return data[training_indices], labels[training_indices], data[validation_indices], labels[validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "def train(X, y, hyperparameters):\n",
    "    training_data, training_labels, validation_data, validation_labels = split_validation(X, y, 0.2)\n",
    "    weights, biases = initialize_layers(training_data.shape[1], hyperparameters['hidden_layers'], training_labels.shape[1])\n",
    "    start = time()\n",
    "    patience = hyperparameters['patience']\n",
    "    reduce_factor = hyperparameters['reduce_factor']\n",
    "    min_lr = hyperparameters['min_lr']\n",
    "    initial_learning_rate = hyperparameters['learning_rate']\n",
    "    learning_rate = initial_learning_rate  \n",
    "    best_val_accuracy = 0  \n",
    "    plateau_count = 0  \n",
    "\n",
    "    for epoch in range(hyperparameters['epochs']):\n",
    "        batches = split_into_batches(training_data, training_labels, hyperparameters['batch_size'])\n",
    "        \n",
    "      \n",
    "        for batch, batch_labels in batches:\n",
    "            weights, biases = train_batch(batch, batch_labels, weights, biases, learning_rate, hyperparameters['dropout_rate'])\n",
    "        \n",
    "        validation_accuracy = test(validation_data, validation_labels, weights, biases) * 100\n",
    "        training_accuracy = test(training_data, training_labels, weights, biases) * 100\n",
    "        duration = time() - start\n",
    "        \n",
    "     \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{hyperparameters[\"epochs\"]} - Validation Accuracy: {validation_accuracy:.2f}% Training Accuracy: {training_accuracy:.2f}% | Duration: {duration:.2f}s')\n",
    "\n",
    "     \n",
    "        if validation_accuracy > best_val_accuracy + 0.1:\n",
    "            best_val_accuracy = validation_accuracy\n",
    "            plateau_count = 0  \n",
    "            save_model(weights, biases, epoch + 1)\n",
    "        else:\n",
    "            if best_val_accuracy >= 95:\n",
    "                print(f'Early stopping at epoch {epoch+1}. Best validation accuracy: {best_val_accuracy:.2f}%')\n",
    "                break\n",
    "            plateau_count += 1\n",
    "\n",
    "      \n",
    "        if plateau_count >= patience:\n",
    "            new_learning_rate = max(learning_rate * reduce_factor, min_lr)\n",
    "            if new_learning_rate < learning_rate:\n",
    "                learning_rate = new_learning_rate\n",
    "            plateau_count = 0 \n",
    "\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 - Validation Accuracy: 94.01% Training Accuracy: 94.31% | Duration: 37.18s\n",
      "Epoch 10/60 - Validation Accuracy: 94.86% Training Accuracy: 95.30% | Duration: 73.25s\n",
      "Epoch 15/60 - Validation Accuracy: 95.00% Training Accuracy: 95.53% | Duration: 109.06s\n",
      "Early stopping at epoch 15. Best validation accuracy: 95.17%\n",
      "Test Accuracy: 95.32%\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    learning_rate=0.9,        \n",
    "    epochs=60,                 \n",
    "    batch_size=128,          \n",
    "    hidden_layers=[100],      \n",
    "    dropout_rate=0.05,         \n",
    "    patience=2,               \n",
    "    reduce_factor=0.5,        \n",
    "    min_lr=5e-3               \n",
    ")\n",
    "\n",
    "model_weight, model_biases= train(train_X,train_Y,hyperparameters)\n",
    "save_model(model_weight, model_biases, hyperparameters['epochs'])\n",
    "test_accuracy = test(test_X, test_Y, model_weight, model_biases) * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
